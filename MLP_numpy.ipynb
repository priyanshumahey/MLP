{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron in Numpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple modular multilayer perceptron with one hidden layer built with numpy. It's designed to be very easy to use and apply."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test dataset showcased here is the mnist numbers dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import numbers_dataset, visualize\n",
    "\n",
    "train_set, test_set, train_labels, test_labesl = numbers_dataset(download=False)\n",
    "\n",
    "print(f'Training datset length: {len(train_set)}')\n",
    "print(f'Testing datset length: {len(test_set)}')\n",
    "\n",
    "visualize(train_set, train_labels, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layered Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function:\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    \"\"\"\n",
    "    Customizable multi-layered pereceptron \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activation_function, inputs, true_output, hidden_layer_size):\n",
    "        self.activation_function = activation_function\n",
    "        self.inputs = inputs\n",
    "        self.weights_1, self.weights_2 = self.weights(inputs, true_output, hidden_layer_size)\n",
    "        self.biases_1, self.biases_2 = self.biases(inputs, hidden_layer_size)\n",
    "        self.output_dim = np.shape(true_output)\n",
    "\n",
    "    def one_hot(self, true_output):\n",
    "        one_hot_matrix = np.zeros((true_output.size, true_output.max()+1), dtype=int)\n",
    "        one_hot_matrix[np.arange(true_output.size),true_output] = 1 \n",
    "        return one_hot_matrix\n",
    "    \n",
    "    def one_hot_to_number(self, one_hot_vector):\n",
    "        return np.argmax(one_hot_vector)\n",
    "\n",
    "    def weights(inputs, true_output, hidden_layer_size):\n",
    "        true_output = len(true_output)\n",
    "        weight_1 = np.random.rand(hidden_layer_size, inputs) - 0.5\n",
    "        weight_2 = np.random.rand(hidden_layer_size, true_output) - 0.5\n",
    "        return weight_1, weight_2\n",
    "\n",
    "    def biases(hidden_layer_size, true_output):\n",
    "        biases_1 = np.random.rand(hidden_layer_size, 1) - 0.5\n",
    "        biases_2 = np.random.rand(len(true_output), 1) - 0.5\n",
    "        return biases_1, biases_2\n",
    "\n",
    "    def layer_in_to_hid(self):\n",
    "        y_hat_1 = np.dot(self.weights_1, self.inputs) + self.biases_1\n",
    "        return self.activation_function(y_hat_1)\n",
    "\n",
    "    def layer_hid_to_out(self):\n",
    "        y_hat_2 = np.dot(self.weights_2, self.layer_in_to_hid()) + self.biases(self.output_dim)\n",
    "        return self.activation_function(y_hat_2)\n",
    "    \n",
    "    # Feed forward\n",
    "    def feed_forward(self, weights, inputs, biases, activate=False):\n",
    "        if activate:\n",
    "            return self.activation_function(np.dot(weights, inputs) + biases)\n",
    "        else:\n",
    "            return np.dot(weights, inputs) + biases\n",
    "        \n",
    "    def full_feed_forward(self):\n",
    "        in_to_hidden = self.activation_function(np.dot(self.weights_1, self.inputs) + self.biases_1)\n",
    "        hidden_to_output = self.activation_function(np.dot(self.weights_2, in_to_hidden) + self.biases_2)\n",
    "        return hidden_to_output\n",
    "\n",
    "    # Back propogation\n",
    "    def back_prop(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(func, data):\n",
    "    return func(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.88079708, 0.95257413])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activate(sigmoid, np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer():\n",
    "    def __init__(self, inputs, n_neurons):\n",
    "        self.weight = np.random.rand(inputs, n_neurons) - 0.5\n",
    "        self.bias = np.random.rand(n_neurons, 1) - 0.5\n",
    "    \n",
    "    def feed_forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weight) + self.bias\n",
    "        return self.output\n",
    "    \n",
    "    def feed_forward_activate(self, inputs, activate):\n",
    "        self.output = activate(np.dot(inputs, self.weight) + self.bias)\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30324976]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1 = DenseLayer(3,1)\n",
    "layer2 = DenseLayer(3, 1)\n",
    "layer1.feed_forward_activate(np.array([1,2,3]), sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
