{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron in Numpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple modular multilayer perceptron with one hidden layer built with numpy. It's designed to be very easy to use and apply."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test dataset showcased here is the mnist numbers dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import numbers_dataset, visualize\n",
    "\n",
    "train_set, test_set, train_labels, test_labesl = numbers_dataset(download=False)\n",
    "\n",
    "print(f'Training datset length: {len(train_set)}')\n",
    "print(f'Testing datset length: {len(test_set)}')\n",
    "\n",
    "visualize(train_set, train_labels, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[0].shape\n",
    "\n",
    "ex1 = train_set[:2]\n",
    "\n",
    "ex1.reshape(1, 2, 784)\n",
    "\n",
    "ex1[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layered Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function:\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biases(hidden_layer_size, true_output):\n",
    "        biases_1 = np.random.rand(hidden_layer_size, 1) - 0.5\n",
    "        biases_2 = np.random.rand(len(true_output), 1) - 0.5\n",
    "        return biases_1, biases_2\n",
    "\n",
    "biases_1, biases_2 = biases(10, [10,10,10,10,10,10,10,10,10,10])\n",
    "\n",
    "biases_1.shape, biases_2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights(inputs, true_output, hidden_layer_size):\n",
    "        weight_1 = np.random.rand(hidden_layer_size, inputs) - 0.5\n",
    "        weight_2 = np.random.rand(hidden_layer_size, true_output) - 0.5\n",
    "        return weight_1, weight_2\n",
    "\n",
    "weight_1, weight_2 = weights(784, 10, 10)\n",
    "weight_1.shape, weight_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = np.random.rand(784, 1)\n",
    "y_output = np.random.rand(10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    \"\"\"\n",
    "    Customizable multi-layered pereceptron \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activation_function, inputs, true_output, hidden_layer_size):\n",
    "        self.activation_function = activation_function\n",
    "        self.input = inputs\n",
    "        self.weights_1, self.weights_2 = self.weights(inputs, true_output, hidden_layer_size)\n",
    "        self.biases_1 = self.biases(inputs, hidden_layer_size)\n",
    "        self.output_dim = np.shape(true_output)\n",
    "        return\n",
    "    \n",
    "    def weights(inputs, true_output, hidden_layer_size):\n",
    "        true_output = len(true_output)\n",
    "        weight_1 = np.random.rand(hidden_layer_size, inputs) - 0.5\n",
    "        weight_2 = np.random.rand(hidden_layer_size, true_output) - 0.5\n",
    "        return weight_1, weight_2\n",
    "\n",
    "\n",
    "    def biases(hidden_layer_size, true_output):\n",
    "        biases_1 = np.random.rand(hidden_layer_size, 1) - 0.5\n",
    "        biases_2 = np.random.rand(len(true_output), 1) - 0.5\n",
    "        return biases_1, biases_2\n",
    "\n",
    "    def layer_in_to_hid(self):\n",
    "        y_hat_1 = np.dot(self.weights_1, self.inputs) + self.biases_1\n",
    "        return self.activation_function(y_hat_1)\n",
    "\n",
    "    def layer_hid_to_out(self):\n",
    "        y_hat_2 = np.dot(self.weights_2, self.layer_in_to_hid()) + self.biases(self.output_dim)\n",
    "        return self.activation_function(y_hat_2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
