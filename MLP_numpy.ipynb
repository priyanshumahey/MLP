{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron in Numpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple modular multilayer perceptron with one hidden layer built with numpy. It's designed to be very easy to use and apply."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test dataset showcased here is the mnist numbers dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import numbers_dataset, visualize\n",
    "\n",
    "train_set, test_set, train_labels, test_labesl = numbers_dataset(download=False)\n",
    "\n",
    "print(f'Training datset length: {len(train_set)}')\n",
    "print(f'Testing datset length: {len(test_set)}')\n",
    "\n",
    "visualize(train_set, train_labels, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function:\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    def __init__(self, activation_function, inputs, true_output):\n",
    "        self.activation_function = activation_function\n",
    "        self.input = inputs\n",
    "        self.weights_1 = self.weights(inputs)\n",
    "        self.biases_1 = self.biases(input)\n",
    "        self.output_dim = np.shape(true_output)\n",
    "        return\n",
    "    \n",
    "    def weights(x):\n",
    "        return x\n",
    "\n",
    "    def biases(x):\n",
    "        return x\n",
    "\n",
    "    def layer_in_to_hid(self):\n",
    "        y_hat_1 = np.matmul(self.weights_1, self.inputs) + self.biases_1\n",
    "        return self.activation_function(y_hat_1)\n",
    "\n",
    "    def layer_hid_to_out(self):\n",
    "        y_hat_2 = np.matmul(self.weights_2, self.layer_in_to_hid()) + self.biases(self.output_dim)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
